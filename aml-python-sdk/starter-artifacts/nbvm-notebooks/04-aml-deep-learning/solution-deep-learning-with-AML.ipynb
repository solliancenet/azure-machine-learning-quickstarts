{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a deep learning model\n",
    "In this notebook you will train a deep learning model to classify the descriptions of car components as compliant or non-compliant. \n",
    "\n",
    "Each document in the supplied training data set is a short text description of the component as documented by an authorized technician. \n",
    "The contents include:\n",
    "- Manufacture year of the component (e.g. 1985, 2010)\n",
    "- Condition of the component (poor, fair, good, new)\n",
    "- Materials used in the component (plastic, carbon fiber, steel, iron)\n",
    "\n",
    "The compliance regulations dictate:\n",
    "*Any component manufactured before 1995 or in fair or poor condition or made with plastic or iron is out of compliance.*\n",
    "\n",
    "For example:\n",
    "* Manufactured in 1985 made of steel in fair condition -> **Non-compliant**\n",
    "* Good condition carbon fiber component manufactured in 2010 -> **Compliant**\n",
    "* Steel component manufactured in 1995 in fair condition -> **Non-Compliant**\n",
    "\n",
    "The labels present in this data are 0 for compliant, 1 for non-compliant.\n",
    "\n",
    "The challenge with classifying text data is that deep learning models only undertand vectors (e.g., arrays of numbers) and not text. To encode the car component descriptions as vectors, we use an algorithm from Stanford called [GloVe (Global Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/). GloVe provides us pre-trained vectors that we can use to convert a string of text into a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n",
    "\n",
    "**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "workspace_region = \"eastus\" # <- region of your Quick-Starts resource group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'deep-learning'\n",
    "project_folder = './dl'\n",
    "deployment_folder = './deploy'\n",
    "\n",
    "# this is the URL to the CSV file containing the GloVe vectors\n",
    "glove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "             'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "             'quickstarts/connected-car-data/glove.6B.100d.txt')\n",
    "\n",
    "# this is the URL to the CSV file containing the care component descriptions\n",
    "data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "            'quickstarts/connected-car-data/connected-car_components.csv')\n",
    "\n",
    "# this is the name of the AML Compute cluster\n",
    "cluster_name = \"gpucluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Azure Machine Learning resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Machine Learning SDK provides a comprehensive set of a capabilities that you can use directly within a notebook including:\n",
    "- Creating a **Workspace** that acts as the root object to organize all artifacts and resources used by Azure Machine Learning.\n",
    "- Creating **Experiments** in your Workspace that capture versions of the trained model along with any desired model performance telemetry. Each time you train a model and evaluate its results, you can capture that run (model and telemetry) within an Experiment.\n",
    "- Creating **Compute** resources that can be used to scale out model training, so that while your notebook may be running in a lightweight container in Azure Notebooks, your model training can actually occur on a powerful cluster that can provide large amounts of memory, CPU or GPU. \n",
    "- Using **Automated Machine Learning (AutoML)** to automatically train multiple versions of a model using a mix of different ways to prepare the data and different algorithms and hyperparameters (algorithm settings) in search of the model that performs best according to a performance metric that you specify.\n",
    "- Packaging a Docker **Image** that contains everything your trained model needs for scoring (prediction) in order to run as a web service.\n",
    "- Deploying your Image to either Azure Kubernetes or Azure Container Instances, effectively hosting the **Web Service**.\n",
    "\n",
    "In Azure Notebooks, all of the libraries needed for Azure Machine Learning are pre-installed. To use them, you just need to import them. Run the following cell to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and connect to an Azure Machine Learning Workspace\n",
    "Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "# By using the exist_ok param, if the worskpace already exists you get a reference to the existing workspace\n",
    "# allowing you to re-run this cell multiple times as desired (which is fairly common in notebooks).\n",
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute Cluster\n",
    "Now you are ready to create the GPU compute cluster. Run the following cell to create a new compute cluster (or retrieve the existing cluster if it already exists). The code below will create a *GPU based* cluster where each node in the cluster is of the size `Standard_NC12`, and the cluster is restricted to use 1 node. This will take couple of minutes to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Creating\n",
      "Succeeded....................\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 1, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-06-11T14:26:19.661000+00:00', 'errors': None, 'creationTime': '2019-06-11T14:24:00.051463+00:00', 'modifiedTime': '2019-06-11T14:24:31.127631+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': ''}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC12'}\n"
     ]
    }
   ],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC12',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Keras Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.13.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "keras_est = TensorFlow(source_directory=project_folder,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script='train.py',\n",
    "                       conda_packages=['pandas'],\n",
    "                       pip_packages=['keras==2.2.4'], # just add keras through pip\n",
    "                       use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remotely train a deep learning model using the Azure ML Compute\n",
    "In the following cells, you will *not* train the model against the data you just downloaded using the resources provided by Azure Notebooks. Instead, you will deploy an Azure ML Compute cluster that will download the data and use a trainings script to train the model. In other words, all of the training will be performed remotely with respect to this notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create project folder\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dl/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/train.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import models \n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "def get_data():\n",
    "    data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "                'quickstarts/connected-car-data/connected-car_components.csv')\n",
    "    car_components_df = pd.read_csv(data_url)\n",
    "    components = car_components_df[\"text\"].tolist()\n",
    "    labels = car_components_df[\"label\"].tolist()\n",
    "    return { \"components\" : components, \"labels\" : labels }\n",
    "\n",
    "def download_glove():\n",
    "    print(\"Downloading GloVe embeddings...\")\n",
    "    import urllib.request\n",
    "    glove_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "                 'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "                 'quickstarts/connected-car-data/glove.6B.100d.txt')\n",
    "    urllib.request.urlretrieve(glove_url, 'glove.6B.100d.txt')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "download_glove()\n",
    "\n",
    "# Load the car components labeled data\n",
    "print(\"Loading car components data...\")\n",
    "data_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "            'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/'\n",
    "            'quickstarts/connected-car-data/connected-car_components.csv')\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()\n",
    "print(\"Loading car components data completed.\")\n",
    "\n",
    "# split data 60% for trianing, 20% for validation, 20% for test\n",
    "print(\"Splitting data...\")\n",
    "train, validate, test = np.split(car_components_df.sample(frac=1), [int(.6*len(car_components_df)), int(.8*len(car_components_df))])\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(validate.shape)\n",
    "\n",
    "# use the Tokenizer from Keras to \"learn\" a vocabulary from the entire car components text\n",
    "print(\"Tokenizing data...\")\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]\n",
    "print(\"Tokenizing data complete.\")\n",
    "\n",
    "# apply the vectors provided by GloVe to create a word embedding matrix\n",
    "print(\"Applying GloVe vectors...\")\n",
    "glove_dir =  './'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector    \n",
    "print(\"Applying GloVe vectors compelted.\")\n",
    "\n",
    "# use Keras to define the structure of the deep neural network   \n",
    "print(\"Creating model structure...\")\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# fix the weights for the first layer to those provided by the embedding matrix\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "print(\"Creating model structure completed.\")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=1, #limit the demo to 1 epoch\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "print(\"Training model completed.\")\n",
    "\n",
    "print(\"Saving model files...\")\n",
    "# create a ./outputs/model folder in the compute target\n",
    "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
    "os.makedirs('./outputs/model', exist_ok=True)\n",
    "# save model\n",
    "model.save('./outputs/model/model.h5')\n",
    "print(\"model saved in ./outputs/model folder\")\n",
    "print(\"Saving model files completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(keras_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the run to complete by executing the following cell. Note that this process will perform the following:\n",
    "- Build and deploy the container to Azure Machine Learning compute (~8 minutes)\n",
    "- Execute the training script (~2 minutes)\n",
    "\n",
    "If you change only the training script and re-submit, it will run faster the second time because the necessary container is already prepared so the time requried is just that for executing the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: deep-learning_1560263497_8dd19aee\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/fdbba0bc-f686-4b8b-8b29-394e0d9ae697/resourceGroups/Quick-Starts-Labs/providers/Microsoft.MachineLearningServices/workspaces/pipelines-workspace/experiments/deep-learning/runs/deep-learning_1560263497_8dd19aee\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "\n",
      "2019/06/11 14:32:50 Downloading source code...\n",
      "2019/06/11 14:32:52 Finished downloading source code\n",
      "2019/06/11 14:32:53 Using acb_vol_3977fb50-9c3b-43a6-a13c-804a18bd0195 as the home volume\n",
      "2019/06/11 14:32:53 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2019/06/11 14:32:53 Successfully set up Docker network: acb_default_network\n",
      "2019/06/11 14:32:53 Setting up Docker configuration...\n",
      "2019/06/11 14:32:54 Successfully set up Docker configuration\n",
      "2019/06/11 14:32:54 Logging in to registry: pipelineswor72a75f36.azurecr.io\n",
      "2019/06/11 14:32:55 Successfully logged into pipelineswor72a75f36.azurecr.io\n",
      "2019/06/11 14:32:55 Executing step ID: acb_step_0. Timeout(sec): 1800, Working directory: '', Network: 'acb_default_network'\n",
      "2019/06/11 14:32:55 Scanning for dependencies...\n",
      "2019/06/11 14:32:56 Successfully scanned dependencies\n",
      "2019/06/11 14:32:56 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  46.08kB\n",
      "\n",
      "Step 1/14 : FROM mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04@sha256:4abc11085c9bbb4e31b1a8e5f6a30d23d92a1d9ff4349b0303d6643142c674de\n",
      "sha256:4abc11085c9bbb4e31b1a8e5f6a30d23d92a1d9ff4349b0303d6643142c674de: Pulling from azureml/base-gpu\n",
      "34667c7e4631: Already exists\n",
      "d18d76a881a4: Already exists\n",
      "119c7358fbfc: Already exists\n",
      "2aaf13f3eff0: Already exists\n",
      "28d5148dfcec: Pulling fs layer\n",
      "454bc542fc4c: Pulling fs layer\n",
      "369f77cbea49: Pulling fs layer\n",
      "ac4ef821cc62: Pulling fs layer\n",
      "9b9781a46f34: Pulling fs layer\n",
      "ade089defcf2: Pulling fs layer\n",
      "a072665bda97: Pulling fs layer\n",
      "9955ff97f151: Pulling fs layer\n",
      "bb20184b6365: Pulling fs layer\n",
      "ede42a37d088: Pulling fs layer\n",
      "290266029613: Pulling fs layer\n",
      "9b9781a46f34: Waiting\n",
      "ade089defcf2: Waiting\n",
      "a072665bda97: Waiting\n",
      "ac4ef821cc62: Waiting\n",
      "9955ff97f151: Waiting\n",
      "bb20184b6365: Waiting\n",
      "ede42a37d088: Waiting\n",
      "290266029613: Waiting\n",
      "28d5148dfcec: Verifying Checksum\n",
      "28d5148dfcec: Download complete\n",
      "369f77cbea49: Verifying Checksum\n",
      "369f77cbea49: Download complete\n",
      "28d5148dfcec: Pull complete\n",
      "454bc542fc4c: Verifying Checksum\n",
      "454bc542fc4c: Download complete\n",
      "454bc542fc4c: Pull complete\n",
      "369f77cbea49: Pull complete\n",
      "9b9781a46f34: Verifying Checksum\n",
      "9b9781a46f34: Download complete\n",
      "a072665bda97: Verifying Checksum\n",
      "a072665bda97: Download complete\n",
      "9955ff97f151: Verifying Checksum\n",
      "9955ff97f151: Download complete\n",
      "bb20184b6365: Download complete\n",
      "ede42a37d088: Verifying Checksum\n",
      "ede42a37d088: Download complete\n",
      "290266029613: Verifying Checksum\n",
      "290266029613: Download complete\n",
      "ade089defcf2: Verifying Checksum\n",
      "ade089defcf2: Download complete\n",
      "ac4ef821cc62: Verifying Checksum\n",
      "ac4ef821cc62: Download complete\n",
      "ac4ef821cc62: Pull complete\n",
      "9b9781a46f34: Pull complete\n",
      "ade089defcf2: Pull complete\n",
      "a072665bda97: Pull complete\n",
      "9955ff97f151: Pull complete\n",
      "bb20184b6365: Pull complete\n",
      "ede42a37d088: Pull complete\n",
      "290266029613: Pull complete\n",
      "Digest: sha256:4abc11085c9bbb4e31b1a8e5f6a30d23d92a1d9ff4349b0303d6643142c674de\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04@sha256:4abc11085c9bbb4e31b1a8e5f6a30d23d92a1d9ff4349b0303d6643142c674de\n",
      " ---> 9c7c162b6422\n",
      "Step 2/14 : USER root\n",
      " ---> Running in 0d608c65e7c5\n",
      "Removing intermediate container 0d608c65e7c5\n",
      " ---> 749eac7ee9cd\n",
      "Step 3/14 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 1dc292849d38\n",
      "Removing intermediate container 1dc292849d38\n",
      " ---> 263238c84975\n",
      "Step 4/14 : WORKDIR /\n",
      " ---> Running in 90a6c8975455\n",
      "Removing intermediate container 90a6c8975455\n",
      " ---> f705a3db3dc6\n",
      "Step 5/14 : COPY azureml-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> c831acc92607\n",
      "Step 6/14 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in b26e5242dd00\n",
      "Removing intermediate container b26e5242dd00\n",
      " ---> 49f3a5ae8bbf\n",
      "Step 7/14 : COPY azureml-setup/mutated_conda_dependencies.yml azureml-setup/mutated_conda_dependencies.yml\n",
      " ---> fc6b0cd6901b\n",
      "Step 8/14 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e -f azureml-setup/mutated_conda_dependencies.yml && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in f6eca11b1218\n",
      "Solving environment: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.6.14\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-3.0.0    | 281 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "libgfortran-3.0.0    | 281 KB    | #########7 |  98% \u001b[0m\u001b[91m\n",
      "libgfortran-3.0.0    | 281 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "pip-19.1.1           | 1.8 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "pip-19.1.1           | 1.8 MB    | #######8   |  79% \u001b[0m\u001b[91m\n",
      "pip-19.1.1           | 1.8 MB    | #########3 |  93% \u001b[0m\u001b[91m\n",
      "pip-19.1.1           | 1.8 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "pandas-0.24.2        | 11.1 MB   |            |   0% \u001b[0m\u001b[91m\n",
      "pandas-0.24.2        | 11.1 MB   | ##8        |  28% \u001b[0m\u001b[91m\n",
      "pandas-0.24.2        | 11.1 MB   | #######5   |  75% \u001b[0m\u001b[91m\n",
      "pandas-0.24.2        | 11.1 MB   | ########9  |  90% \u001b[0m\u001b[91m\n",
      "pandas-0.24.2        | 11.1 MB   | #########9 | 100% \u001b[0m\u001b[91m\n",
      "pandas-0.24.2        | 11.1 MB   | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "liblapack-3.8.0      | 6 KB      |            |   0% \u001b[0m\u001b[91m\n",
      "liblapack-3.8.0      | 6 KB      | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "zlib-1.2.11          | 101 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "zlib-1.2.11          | 101 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "libblas-3.8.0        | 6 KB      |            |   0% \u001b[0m\u001b[91m\n",
      "libblas-3.8.0        | 6 KB      | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "numpy-1.16.4         | 4.3 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "numpy-1.16.4         | 4.3 MB    | #######5   |  76% \u001b[0m\u001b[91m\n",
      "numpy-1.16.4         | 4.3 MB    | #########8 |  98% \u001b[0m\u001b[91m\n",
      "numpy-1.16.4         | 4.3 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "tk-8.5.19            | 1.9 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | #######7   |  78% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | #########  |  91% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | #######5   |  75% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | #########7 |  98% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "ncurses-5.9          | 1.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | #######8   |  79% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | ########6  |  87% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | #########7 |  97% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "ca-certificates-2019 | 146 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "ca-certificates-2019 | 146 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "six-1.12.0           | 22 KB     |            |   0% \u001b[0m\u001b[91m\n",
      "six-1.12.0           | 22 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "wheel-0.33.4         | 34 KB     |            |   0% \u001b[0m\u001b[91m\n",
      "wheel-0.33.4         | 34 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "openssl-1.0.2r       | 3.1 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | #######7   |  77% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "sqlite-3.13.0        | 4.9 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | #######6   |  76% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | #########9 |  99% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "readline-6.2         | 713 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "readline-6.2         | 713 KB    | ########4  |  85% \u001b[0m\u001b[91m\n",
      "readline-6.2         | 713 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "openblas-0.3.5       | 13.7 MB   |            |   0% \u001b[0m\u001b[91m\n",
      "openblas-0.3.5       | 13.7 MB   | ##4        |  24% \u001b[0m\u001b[91m\n",
      "openblas-0.3.5       | 13.7 MB   | #######5   |  75% \u001b[0m\u001b[91m\n",
      "openblas-0.3.5       | 13.7 MB   | #########5 |  95% \u001b[0m\u001b[91m\n",
      "openblas-0.3.5       | 13.7 MB   | ########## | 100% \u001b[0m\u001b[91m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pytz-2019.1          | 227 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "pytz-2019.1          | 227 KB    | #########4 |  94% \u001b[0m\u001b[91m\n",
      "pytz-2019.1          | 227 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "python-3.6.2         | 19.0 MB   |            |   0% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ##1        |  22% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | #####3     |  54% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | #######5   |  75% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ########8  |  88% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | #########7 |  97% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "setuptools-41.0.1    | 612 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "setuptools-41.0.1    | 612 KB    | ########5  |  85% \u001b[0m\u001b[91m\n",
      "setuptools-41.0.1    | 612 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "python-dateutil-2.8. | 219 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "python-dateutil-2.8. | 219 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "libcblas-3.8.0       | 6 KB      |            |   0% \u001b[0m\u001b[91m\n",
      "libcblas-3.8.0       | 6 KB      | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "xz-5.2.4             | 366 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "xz-5.2.4             | 366 KB    | #########4 |  95% \u001b[0m\u001b[91m\n",
      "xz-5.2.4             | 366 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \u001b[0m\u001b[91m\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | #######6   |  77% \u001b[0m\u001b[91m\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | #########5 |  96% \u001b[0m\u001b[91m\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "\n",
      "certifi-2019.3.9     | 149 KB    |            |   0% \u001b[0m\u001b[91m\n",
      "certifi-2019.3.9     | 149 KB    | ########## | 100% \u001b[0m\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Collecting keras==2.2.4 (from -r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Collecting azureml-defaults (from -r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/2c/f4/c34cb32d3c789e5205d9bc1a9b7f191f3dfe45a9fab5655d7441c198f63b/azureml_defaults-1.0.43-py2.py3-none-any.whl\n",
      "Collecting tensorflow-gpu==1.13.1 (from -r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
      "Collecting horovod==0.16.1 (from -r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/89/70/327e1ce9bee0fb8a879b98f8265fb7a41ae6d04a3ee019b2bafba8b66333/horovod-0.16.1.tar.gz (2.6MB)\n",
      "Collecting scipy>=0.14 (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/72/4c/5f81e7264b0a7a8bd570810f48cd346ba36faedbd2ba255c873ad556de76/scipy-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (25.2MB)\n",
      "Collecting h5py (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting pyyaml (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
      "Collecting keras-applications>=1.0.6 (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1)) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from keras==2.2.4->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 1)) (1.12.0)\n",
      "Collecting azureml-core==1.0.43.* (from azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/62/9a09407b80ce1ebfc678c1b5b3c828faa4bd88dd3b8b2ea5453b4904ef37/azureml_core-1.0.43-py2.py3-none-any.whl (937kB)\n",
      "Collecting applicationinsights>=0.11.7 (from azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/53/234c53004f71f0717d8acd37876e0b65c121181167057b9ce1b1795f96a0/applicationinsights-0.11.9-py2.py3-none-any.whl (58kB)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: wheel>=0.26 in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3)) (0.33.4)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/d2/fb/29de8d08967f0cce1bb10b39846d836b0f3bf6776ddc36aed7c73498ca7e/protobuf-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/99/83/18f374294bf34128a448ee2fae37651f943b0b5fa473b5b3aff262c15bf8/grpcio-1.21.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "Collecting cffi>=1.4.0 (from horovod==0.16.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/bf/6aa1925384c23ffeb579e97a5569eb9abce41b6310b329352b8252cee1c3/cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl (430kB)\n",
      "Collecting cloudpickle (from horovod==0.16.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
      "Collecting psutil (from horovod==0.16.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/ca/5b8c1fe032a458c2c4bcbe509d1401dca9dda35c7fc46b36bb81c2834740/psutil-5.6.3.tar.gz (435kB)\n",
      "Collecting backports.tempfile (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/5c/077f910632476281428fe254807952eb47ca78e720d059a46178c541e669/backports.tempfile-1.0-py2.py3-none-any.whl\n",
      "Collecting requests>=2.19.1 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting msrestazure>=0.4.33 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/aa/b17a4f702ecd6d9e989ae34109aa384c988aed0de37215c651165ed45238/msrestazure-0.6.1-py2.py3-none-any.whl (40kB)\n",
      "Collecting azure-mgmt-resource>=1.2.1 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/2d/9c3f2c9bf6e19f3efe4ddcaa946ea515d5229f4c0b387e224267e77fb73b/azure_mgmt_resource-2.2.0-py2.py3-none-any.whl (780kB)\n",
      "Collecting jsonpickle (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-keyvault>=0.40.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/49/de/0d69aedae7c5f6428314640b65947203ab80409c12b5d4e66fb5b7a4182e/azure_mgmt_keyvault-1.1.0-py2.py3-none-any.whl (111kB)\n",
      "Collecting azure-graphrbac>=0.40.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/93/02056aca45162f9fc275d1eaad12a2a07ef92375afb48eabddc4134b8315/azure_graphrbac-0.61.1-py2.py3-none-any.whl (141kB)\n",
      "Collecting pyopenssl (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
      "Collecting docker (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/91/93/310fe092039f6b0759a1f8524e9e2c56f8012804fa2a8da4e4289bb74d7c/docker-4.0.1-py2.py3-none-any.whl (138kB)\n",
      "Collecting PyJWT (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
      "Collecting urllib3>=1.23 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "Collecting azure-common>=1.1.12 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/36/9946fa617f458f11766884c76c622810f4c111ee16c08eb8315e88330d66/azure_common-1.1.22-py2.py3-none-any.whl\n",
      "Collecting contextlib2 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/71/8273a7eeed0aff6a854237ab5453bc9aa67deb49df4832801c21f0ff3782/contextlib2-0.5.5-py2.py3-none-any.whl\n",
      "Collecting ruamel.yaml<=0.15.89,>=0.15.35 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/36/e1/cc2fa400fa5ffde3efa834ceb15c464075586de05ca3c553753dcd6f1d3b/ruamel.yaml-0.15.89-cp36-cp36m-manylinux1_x86_64.whl (651kB)\n",
      "Collecting ndg-httpsclient (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/67/c2f508c00ed2a6911541494504b7cac16fe0b0473912568df65fd1801132/ndg_httpsclient-0.5.1-py3-none-any.whl\n",
      "Collecting adal>=1.2.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/00/72/53dce9e4f5d6c1aa57b8d408cb34dff1969ecbf10ab7e678f32c5e0e2397/adal-1.2.1-py2.py3-none-any.whl (52kB)\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/97/70/8c2d0509db466678eba16fa2b0a539499f3b351b1f2993126ad843d5be13/azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718kB)\n",
      "Requirement already satisfied: pytz in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2)) (2019.1)\n",
      "Collecting paramiko>=2.0.8 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/17/9f/7430d1ed509e195d5a5bb1a2bda6353a4aa64eb95491f198a17c44e2075c/paramiko-2.5.0-py2.py3-none-any.whl (198kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
      "Collecting SecretStorage (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\n",
      "Collecting azure-mgmt-storage>=1.5.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/99/c0cd1caf54c89c81973acc87b2f9a8fa4c5ce3c5e14f9be61a23cf335276/azure_mgmt_storage-3.3.0-py2.py3-none-any.whl (1.0MB)\n",
      "Collecting jmespath (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-authorization>=0.40.0 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/b2/c0d62a3a91c13641e09af294c13fe16929f88dc5902718388cd9b292217f/azure_mgmt_authorization-0.52.0-py2.py3-none-any.whl (112kB)\n",
      "Collecting msrest>=0.5.1 (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/fa/bb4197e25ff01d089dc0584ad8e7d6c2615ae28b9e850afd165927c89576/msrest-0.6.6-py2.py3-none-any.whl (81kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2)) (2.8.0)\n",
      "Collecting pathspec (from azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/2a/bfee636b1e2f7d6e30dd74f49201ccfa5c3cf322d44929ecc6c137c486c5/pathspec-0.5.9.tar.gz\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/57/92a497e38161ce40606c27a86759c6b92dd34fcdb33f64171ec559257c02/Werkzeug-0.15.4-py2.py3-none-any.whl (327kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: setuptools in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3)) (41.0.1)\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting pycparser (from cffi>=1.4.0->horovod==0.16.1->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
      "Collecting backports.weakref (from backports.tempfile->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests>=2.19.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages (from requests>=2.19.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2)) (2019.3.9)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.19.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting azure-mgmt-nspkg>=2.0.0 (from azure-mgmt-keyvault>=0.40.0->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/c2/af4b47845f27dc7d206ed4908b9e580f8bc94a4b2f3956a0d87c40719d90/azure_mgmt_nspkg-3.0.2-py3-none-any.whl\n",
      "Collecting websocket-client>=0.32.0 (from docker->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
      "Collecting pyasn1>=0.1.1 (from ndg-httpsclient->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/7c/c9386b82a25115cccf1903441bba3cbadcfae7b678a20167347fa8ded34c/pyasn1-0.4.5-py2.py3-none-any.whl (73kB)\n",
      "Collecting pynacl>=1.0.1 (from paramiko>=2.0.8->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/27/15/2cd0a203f318c2240b42cd9dd13c931ddd61067809fee3479f44f086103e/PyNaCl-1.3.0-cp34-abi3-manylinux1_x86_64.whl (759kB)\n",
      "Collecting bcrypt>=3.1.3 (from paramiko>=2.0.8->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/79/79a4d167a31cc206117d9b396926615fa9c1fdbd52017bcced80937ac501/bcrypt-3.1.6-cp34-abi3-manylinux1_x86_64.whl (55kB)\n",
      "Collecting asn1crypto>=0.21.0 (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
      "Collecting jeepney (from SecretStorage->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/f7/ff23b9b59534f501d47c327576aadda59da5b83d76ff837e6075bc325b9f/jeepney-0.4-py3-none-any.whl (59kB)\n",
      "Collecting isodate>=0.6.0 (from msrest>=0.5.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "Collecting requests-oauthlib>=0.5.0 (from msrest>=0.5.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\n",
      "Collecting azure-nspkg>=3.0.0 (from azure-mgmt-nspkg>=2.0.0->azure-mgmt-keyvault>=0.40.0->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/0c/c562be95a9a2ed52454f598571cf300b1114d0db2aa27f5b8ed3bb9cd0c0/azure_nspkg-3.0.2-py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-core==1.0.43.*->azureml-defaults->-r /azureml-setup/condaenv.vcxbk83p.requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/16/95/699466b05b72b94a41f662dc9edf87fda4289e3602ecd42d27fcaddf7b56/oauthlib-3.0.1-py2.py3-none-any.whl (142kB)\n",
      "Building wheels for collected packages: horovod, pyyaml, absl-py, termcolor, gast, psutil, pathspec, pycparser\n",
      "  Building wheel for horovod (setup.py): started\n",
      "  Building wheel for horovod (setup.py): finished with status 'error'\n",
      "\u001b[91m  ERROR: Complete output from command /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/bin/python -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-8rqiudky/horovod/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-hf_xq726 --python-tag cp36:\n",
      "  ERROR: \n",
      "  Installed /tmp/pip-install-8rqiudky/horovod/.eggs/psutil-5.6.3-py3.6-linux-x86_64.egg\n",
      "  Searching for cloudpickle\n",
      "  Reading https://pypi.org/simple/cloudpickle/\n",
      "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl#sha256=b8ba7e322f2394b9bbbdc1c976e6442c2c02acc784cb9e553cee9186166a6890\n",
      "  Best match: cloudpickle 1.2.1\n",
      "  Processing cloudpickle-1.2.1-py2.py3-none-any.whl\n",
      "  Installing cloudpickle-1.2.1-py2.py3-none-any.whl to /tmp/pip-install-8rqiudky/horovod/.eggs\n",
      "  \n",
      "  Installed /tmp/pip-install-8rqiudky/horovod/.eggs/cloudpickle-1.2.1-py3.6.egg\n",
      "  Searching for cffi>=1.4.0\n",
      "  Reading https://pypi.org/simple/cffi/\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/bf/6aa1925384c23ffeb579e97a5569eb9abce41b6310b329352b8252cee1c3/cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl#sha256=59b4dc008f98fc6ee2bb4fd7fc786a8d70000d058c2bbe2698275bc53a8d3fa7\n",
      "  Best match: cffi 1.12.3\n",
      "  Processing cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "  Installing cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl to /tmp/pip-install-8rqiudky/horovod/.eggs\n",
      "  writing requirements to /tmp/pip-install-8rqiudky/horovod/.eggs/cffi-1.12.3-py3.6-linux-x86_64.egg/EGG-INFO/requires.txt\n",
      "  \n",
      "  Installed /tmp/pip-install-8rqiudky/horovod/.eggs/cffi-1.12.3-py3.6-linux-x86_64.egg\n",
      "  Searching for pycparser\n",
      "  Reading https://pypi.org/simple/pycparser/\n",
      "  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz#sha256=a988718abfad80b6b157acce7bf130a30876d27603738ac39f140993246b25b3\n",
      "  Best match: pycparser 2.19\n",
      "  Processing pycparser-2.19.tar.gz\n",
      "  Writing /tmp/easy_install-y491bl1x/pycparser-2.19/setup.cfg\n",
      "  Running pycparser-2.19/setup.py -q bdist_egg --dist-dir /tmp/easy_install-y491bl1x/pycparser-2.19/egg-dist-tmp-faobhfo3\n",
      "  warning: no previously-included files found matching 'setup.pyc'\n",
      "  warning: no previously-included files matching 'yacctab.*' found under directory 'tests'\n",
      "  warning: no previously-included files matching 'lextab.*' found under directory 'tests'\n",
      "  warning: no previously-included files matching 'yacctab.*' found under directory 'examples'\n",
      "  warning: no previously-included files matching 'lextab.*' found under directory 'examples'\n",
      "  zip_safe flag not set; analyzing archive contents...\n",
      "  pycparser.ply.__pycache__.lex.cpython-36: module references __file__\n",
      "  pycparser.ply.__pycache__.lex.cpython-36: module MAY be using inspect.getsourcefile\n",
      "  pycparser.ply.__pycache__.yacc.cpython-36: module references __file__\n",
      "  pycparser.ply.__pycache__.yacc.cpython-36: module MAY be using inspect.getsourcefile\n",
      "  pycparser.ply.__pycache__.yacc.cpython-36: module MAY be using inspect.stack\n",
      "  pycparser.ply.__pycache__.ygen.cpython-36: module references __file__\n",
      "  creating /tmp/pip-install-8rqiudky/horovod/.eggs/pycparser-2.19-py3.6.egg\n",
      "  Extracting pycparser-2.19-py3.6.egg to /tmp/pip-install-8rqiudky/horovod/.eggs\n",
      "  \n",
      "  Installed /tmp/pip-install-8rqiudky/horovod/.eggs/pycparser-2.19-py3.6.egg\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-x86_64-3.6\n",
      "  creating build/lib.linux-x86_64-3.6/horovod\n",
      "  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/run.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  copying horovod/run/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/run\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/common\n",
      "  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark\n",
      "  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common\n",
      "  copying horovod/run/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  copying horovod/run/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  copying horovod/run/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/driver\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  copying horovod/run/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  copying horovod/run/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/task\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  copying horovod/run/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/run/util\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  copying horovod/run/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  copying horovod/run/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\n",
      "  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\n",
      "  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver\n",
      "  creating build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task\n",
      "  running build_ext\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -std=c++11 -fPIC -O2 -Wall -mf16c -mavx -I/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o\n",
      "  cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -Wl,-rpath=/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib,--no-as-needed -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -Wl,-rpath=/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib,--no-as-needed build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.so\n",
      "  gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_link_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o\n",
      "  cc1plus: warning: command line option -Wstrict-prototypes is valid for C/ObjC but not for C++\n",
      "  gcc -pthread -shared -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -Wl,-rpath=/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib,--no-as-needed -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -Wl,-rpath=/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib,--no-as-needed -Wl,--version-script=horovod.lds build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o -L/azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.so\n",
      "  INFO: Unable to build TensorFlow plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 55, in check_tf_version\n",
      "      import tensorflow as tf\n",
      "  ModuleNotFoundError: No module named 'tensorflow'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 909, in build_extensions\n",
      "      build_tf_extension(self, options)\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 608, in build_tf_extension\n",
      "      check_tf_version()\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 62, in check_tf_version\n",
      "      'import tensorflow failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import tensorflow failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 55, in check_tf_version\n",
      "      import tensorflow as tf\n",
      "  ModuleNotFoundError: No module named 'tensorflow'\n",
      "  \n",
      "  \n",
      "  INFO: Unable to build PyTorch plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 711, in check_torch_version\n",
      "      import torch\n",
      "  ModuleNotFoundError: No module named 'torch'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 920, in build_extensions\n",
      "      torch_version = check_torch_version()\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 718, in check_torch_version\n",
      "      'import torch failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import torch failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 711, in check_torch_version\n",
      "      import torch\n",
      "  ModuleNotFoundError: No module named 'torch'\n",
      "  \n",
      "  \n",
      "  INFO: Unable to build MXNet plugin, will skip it.\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 71, in check_mx_version\n",
      "      import mxnet as mx\n",
      "  ModuleNotFoundError: No module named 'mxnet'\n",
      "  \n",
      "  During handling of the above exception, another exception occurred:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 935, in build_extensions\n",
      "      build_mx_extension(self, options)\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 672, in build_mx_extension\n",
      "      check_mx_version()\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 78, in check_mx_version\n",
      "      'import mxnet failed, is it installed?\\n\\n%s' % traceback.format_exc())\n",
      "  distutils.errors.DistutilsPlatformError: import mxnet failed, is it installed?\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"/tmp/pip-install-8rqiudky/horovod/setup.py\", line 71, in check_mx_version\n",
      "      import mxnet as mx\n",
      "  ModuleNotFoundError: No module named 'mxnet'\n",
      "  \n",
      "  \n",
      "  error: None of TensorFlow, PyTorch, or MXNet plugins were built. See errors above.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for horovod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m  Running setup.py clean for horovod\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for psutil (setup.py): started\n",
      "  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/7e/74/bb640d77775e6b6a78bcc3120f9fea4d2a28b2706de1cff37d\n",
      "  Building wheel for pathspec (setup.py): started\n",
      "  Building wheel for pathspec (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/45/cb/7e/ce6e6062c69446e39e328170524ca8213498bc66a74c6a210b\n",
      "  Building wheel for pycparser (setup.py): started\n",
      "  Building wheel for pycparser (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built pyyaml absl-py termcolor gast psutil pathspec pycparser\n",
      "Failed to build horovod\n",
      "Installing collected packages: scipy, h5py, keras-preprocessing, pyyaml, keras-applications, keras, backports.weakref, backports.tempfile, idna, urllib3, chardet, requests, isodate, oauthlib, requests-oauthlib, msrest, PyJWT, asn1crypto, pycparser, cffi, cryptography, adal, msrestazure, azure-common, azure-mgmt-resource, jsonpickle, azure-nspkg, azure-mgmt-nspkg, azure-mgmt-keyvault, azure-graphrbac, pyopenssl, websocket-client, docker, contextlib2, ruamel.yaml, pyasn1, ndg-httpsclient, azure-mgmt-containerregistry, pynacl, bcrypt, paramiko, jeepney, SecretStorage, azure-mgmt-storage, jmespath, azure-mgmt-authorization, pathspec, azureml-core, applicationinsights, azureml-defaults, absl-py, astor, werkzeug, grpcio, protobuf, markdown, tensorboard, termcolor, gast, mock, tensorflow-estimator, tensorflow-gpu, cloudpickle, psutil, horovod\n",
      "  Running setup.py install for horovod: started\n",
      "    Running setup.py install for horovod: finished with status 'done'\n",
      "Successfully installed PyJWT-1.7.1 SecretStorage-3.1.1 absl-py-0.7.1 adal-1.2.1 applicationinsights-0.11.9 asn1crypto-0.24.0 astor-0.8.0 azure-common-1.1.22 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.52.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-1.1.0 azure-mgmt-nspkg-3.0.2 azure-mgmt-resource-2.2.0 azure-mgmt-storage-3.3.0 azure-nspkg-3.0.2 azureml-core-1.0.43 azureml-defaults-1.0.43 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-3.1.6 cffi-1.12.3 chardet-3.0.4 cloudpickle-1.2.1 contextlib2-0.5.5 cryptography-2.7 docker-4.0.1 gast-0.2.2 grpcio-1.21.1 h5py-2.9.0 horovod-0.16.1 idna-2.8 isodate-0.6.0 jeepney-0.4 jmespath-0.9.4 jsonpickle-1.2 keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 mock-3.0.5 msrest-0.6.6 msrestazure-0.6.1 ndg-httpsclient-0.5.1 oauthlib-3.0.1 paramiko-2.5.0 pathspec-0.5.9 protobuf-3.8.0 psutil-5.6.3 pyasn1-0.4.5 pycparser-2.19 pynacl-1.3.0 pyopenssl-19.0.0 pyyaml-5.1.1 requests-2.22.0 requests-oauthlib-1.2.0 ruamel.yaml-0.15.89 scipy-1.3.0 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1 termcolor-1.1.0 urllib3-1.25.3 websocket-client-0.56.0 werkzeug-0.15.4\n",
      "\u001b[91m\n",
      "\u001b[0m#\n",
      "# To activate this environment, use:\n",
      "# > source activate /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e\n",
      "#\n",
      "# To deactivate an active environment, use:\n",
      "# > source deactivate\n",
      "#\n",
      "\n",
      "Removing intermediate container f6eca11b1218\n",
      " ---> 96787c63a275\n",
      "Step 9/14 : ENV PATH /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/bin:$PATH\n",
      " ---> Running in 21c34a0276db\n",
      "Removing intermediate container 21c34a0276db\n",
      " ---> 4097d81737be\n",
      "Step 10/14 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e\n",
      " ---> Running in 8b7f4bff394c\n",
      "Removing intermediate container 8b7f4bff394c\n",
      " ---> d8986058ec21\n",
      "Step 11/14 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in b1e99905262f\n",
      "Removing intermediate container b1e99905262f\n",
      " ---> c1674e1bdff5\n",
      "Step 12/14 : COPY azureml-setup/spark_cache.py azureml-setup/log4j.properties /azureml-setup/\n",
      " ---> fef359a2896d\n",
      "Step 13/14 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 72bd2a457254\n",
      "Removing intermediate container 72bd2a457254\n",
      " ---> 56b652ec3d38\n",
      "Step 14/14 : CMD [\"bash\"]\n",
      " ---> Running in bfc4518192d6\n",
      "Removing intermediate container bfc4518192d6\n",
      " ---> 973384781903\n",
      "Successfully built 973384781903\n",
      "Successfully tagged pipelineswor72a75f36.azurecr.io/azureml/azureml_d637b5dff0e8823965fb22958baf6ae4:latest\n",
      "2019/06/11 14:38:51 Successfully executed container: acb_step_0\n",
      "2019/06/11 14:38:51 Executing step ID: acb_step_1. Timeout(sec): 1800, Working directory: '', Network: 'acb_default_network'\n",
      "2019/06/11 14:38:51 Pushing image: pipelineswor72a75f36.azurecr.io/azureml/azureml_d637b5dff0e8823965fb22958baf6ae4:latest, attempt 1\n",
      "The push refers to repository [pipelineswor72a75f36.azurecr.io/azureml/azureml_d637b5dff0e8823965fb22958baf6ae4]\n",
      "d04bc03e32db: Preparing\n",
      "7e1f16265c38: Preparing\n",
      "eec17ae65a3c: Preparing\n",
      "f91726b03c53: Preparing\n",
      "c210dd067f79: Preparing\n",
      "bd334bfc9a0a: Preparing\n",
      "f0d17acb4a75: Preparing\n",
      "8620c3423651: Preparing\n",
      "2b3f9a1565f2: Preparing\n",
      "57e5dcecfef6: Preparing\n",
      "1e5b24e2c1f5: Preparing\n",
      "5d6cca69c100: Preparing\n",
      "31b946ea17bb: Preparing\n",
      "ef3304f85894: Preparing\n",
      "cacd9b90c818: Preparing\n",
      "221e6befd0c5: Preparing\n",
      "478462fe5e15: Preparing\n",
      "297fd071ca2f: Preparing\n",
      "2f0d1e8214b2: Preparing\n",
      "7dd604ffa87f: Preparing\n",
      "aa54c2bc1229: Preparing\n",
      "bd334bfc9a0a: Waiting\n",
      "f0d17acb4a75: Waiting\n",
      "8620c3423651: Waiting\n",
      "2b3f9a1565f2: Waiting\n",
      "57e5dcecfef6: Waiting\n",
      "1e5b24e2c1f5: Waiting\n",
      "5d6cca69c100: Waiting\n",
      "31b946ea17bb: Waiting\n",
      "ef3304f85894: Waiting\n",
      "cacd9b90c818: Waiting\n",
      "221e6befd0c5: Waiting\n",
      "478462fe5e15: Waiting\n",
      "297fd071ca2f: Waiting\n",
      "2f0d1e8214b2: Waiting\n",
      "7dd604ffa87f: Waiting\n",
      "aa54c2bc1229: Waiting\n",
      "c210dd067f79: Pushed\n",
      "d04bc03e32db: Pushed\n",
      "eec17ae65a3c: Pushed\n",
      "f91726b03c53: Pushed\n",
      "bd334bfc9a0a: Pushed\n",
      "f0d17acb4a75: Pushed\n",
      "8620c3423651: Pushed\n",
      "1e5b24e2c1f5: Pushed\n",
      "2b3f9a1565f2: Pushed\n",
      "57e5dcecfef6: Pushed\n",
      "cacd9b90c818: Pushed\n",
      "5d6cca69c100: Pushed\n",
      "221e6befd0c5: Pushed\n",
      "478462fe5e15: Pushed\n",
      "297fd071ca2f: Pushed\n",
      "2f0d1e8214b2: Pushed\n",
      "7dd604ffa87f: Pushed\n",
      "aa54c2bc1229: Pushed\n",
      "ef3304f85894: Pushed\n",
      "31b946ea17bb: Pushed\n",
      "7e1f16265c38: Pushed\n",
      "latest: digest: sha256:11f28e81898c46b3f008463075a4c278eaa5d71135ece16d74f1e2c0f6ff8b8c size: 4728\n",
      "2019/06/11 14:43:21 Successfully pushed image: pipelineswor72a75f36.azurecr.io/azureml/azureml_d637b5dff0e8823965fb22958baf6ae4:latest\n",
      "2019/06/11 14:43:21 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 355.288066)\n",
      "2019/06/11 14:43:21 Populating digests for step ID: acb_step_0...\n",
      "2019/06/11 14:43:22 Successfully populated digests for step ID: acb_step_0\n",
      "2019/06/11 14:43:22 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 269.822166)\n",
      "2019/06/11 14:43:22 The following dependencies were found:\n",
      "2019/06/11 14:43:22 \n",
      "- image:\n",
      "    registry: pipelineswor72a75f36.azurecr.io\n",
      "    repository: azureml/azureml_d637b5dff0e8823965fb22958baf6ae4\n",
      "    tag: latest\n",
      "    digest: sha256:11f28e81898c46b3f008463075a4c278eaa5d71135ece16d74f1e2c0f6ff8b8c\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/base-gpu\n",
      "    tag: intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04\n",
      "    digest: sha256:4abc11085c9bbb4e31b1a8e5f6a30d23d92a1d9ff4349b0303d6643142c674de\n",
      "  git: {}\n",
      "\n",
      "Run ID: ca8 was successful after 10m33s\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings...\n",
      "Download complete.\n",
      "Loading car components data...\n",
      "Loading car components data completed.\n",
      "Splitting data...\n",
      "(60000, 2)\n",
      "(20000, 2)\n",
      "(20000, 2)\n",
      "Tokenizing data...\n",
      "Found 65 unique tokens.\n",
      "Shape of data tensor: (100000, 100)\n",
      "Shape of label tensor: (100000,)\n",
      "Tokenizing data complete.\n",
      "Applying GloVe vectors...\n",
      "Found 400000 word vectors.\n",
      "Applying GloVe vectors compelted.\n",
      "Creating model structure...\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2019-06-11 14:46:15.183405: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-06-11 14:46:15.526945: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x609db50 executing computations on platform CUDA. Devices:\n",
      "2019-06-11 14:46:15.526980: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-06-11 14:46:15.526988: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7\n",
      "2019-06-11 14:46:15.530012: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n",
      "2019-06-11 14:46:15.531462: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6331050 executing computations on platform Host. Devices:\n",
      "2019-06-11 14:46:15.531484: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-06-11 14:46:15.532030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 9399:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2019-06-11 14:46:15.532503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 1 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: b107:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2019-06-11 14:46:15.532718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0, 1\n",
      "2019-06-11 14:46:15.537582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-06-11 14:46:15.537603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 1 \n",
      "2019-06-11 14:46:15.537611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N N \n",
      "2019-06-11 14:46:15.537617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 1:   N N \n",
      "2019-06-11 14:46:15.538578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 9399:00:00.0, compute capability: 3.7)\n",
      "2019-06-11 14:46:15.538976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10805 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: b107:00:00.0, compute capability: 3.7)\n",
      "Creating model structure completed.\n",
      "Training model...\n",
      "WARNING:tensorflow:From /azureml-envs/azureml_ff7c3bb160bffab623c1ae25aa557f1e/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 90000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "2019-06-11 14:46:16.535007: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "\n",
      "   32/90000 [..............................] - ETA: 16:54 - loss: 0.7038 - acc: 0.4375\n",
      "  704/90000 [..............................] - ETA: 52s - loss: 0.6848 - acc: 0.5398  \n",
      " 1376/90000 [..............................] - ETA: 29s - loss: 0.6458 - acc: 0.6163\n",
      " 2016/90000 [..............................] - ETA: 22s - loss: 0.6010 - acc: 0.6731\n",
      " 2656/90000 [..............................] - ETA: 18s - loss: 0.5684 - acc: 0.7131\n",
      " 3328/90000 [>.............................] - ETA: 16s - loss: 0.5388 - acc: 0.7359\n",
      " 4000/90000 [>.............................] - ETA: 14s - loss: 0.5130 - acc: 0.7592\n",
      " 4672/90000 [>.............................] - ETA: 13s - loss: 0.4851 - acc: 0.7840\n",
      " 5344/90000 [>.............................] - ETA: 12s - loss: 0.4619 - acc: 0.8018\n",
      " 6016/90000 [=>............................] - ETA: 11s - loss: 0.4376 - acc: 0.8185\n",
      " 6688/90000 [=>............................] - ETA: 10s - loss: 0.4166 - acc: 0.8322\n",
      " 7360/90000 [=>............................] - ETA: 10s - loss: 0.3960 - acc: 0.8446\n",
      " 8032/90000 [=>............................] - ETA: 9s - loss: 0.3765 - acc: 0.8553 \n",
      " 8704/90000 [=>............................] - ETA: 9s - loss: 0.3597 - acc: 0.8641\n",
      " 9376/90000 [==>...........................] - ETA: 9s - loss: 0.3438 - acc: 0.8723\n",
      "10048/90000 [==>...........................] - ETA: 9s - loss: 0.3292 - acc: 0.8796\n",
      "10720/90000 [==>...........................] - ETA: 8s - loss: 0.3162 - acc: 0.8861\n",
      "11392/90000 [==>...........................] - ETA: 8s - loss: 0.3033 - acc: 0.8918\n",
      "12064/90000 [===>..........................] - ETA: 8s - loss: 0.2913 - acc: 0.8971\n",
      "12736/90000 [===>..........................] - ETA: 8s - loss: 0.2801 - acc: 0.9022\n",
      "13408/90000 [===>..........................] - ETA: 7s - loss: 0.2691 - acc: 0.9068\n",
      "14080/90000 [===>..........................] - ETA: 7s - loss: 0.2597 - acc: 0.9103\n",
      "14752/90000 [===>..........................] - ETA: 7s - loss: 0.2513 - acc: 0.9136\n",
      "15392/90000 [====>.........................] - ETA: 7s - loss: 0.2434 - acc: 0.9167\n",
      "16064/90000 [====>.........................] - ETA: 7s - loss: 0.2356 - acc: 0.9198\n",
      "16736/90000 [====>.........................] - ETA: 7s - loss: 0.2281 - acc: 0.9227\n",
      "17408/90000 [====>.........................] - ETA: 7s - loss: 0.2210 - acc: 0.9253\n",
      "18048/90000 [=====>........................] - ETA: 6s - loss: 0.2145 - acc: 0.9278\n",
      "18720/90000 [=====>........................] - ETA: 6s - loss: 0.2081 - acc: 0.9303\n",
      "19392/90000 [=====>........................] - ETA: 6s - loss: 0.2021 - acc: 0.9324\n",
      "20064/90000 [=====>........................] - ETA: 6s - loss: 0.1965 - acc: 0.9343\n",
      "20736/90000 [=====>........................] - ETA: 6s - loss: 0.1911 - acc: 0.9362\n",
      "21408/90000 [======>.......................] - ETA: 6s - loss: 0.1861 - acc: 0.9381\n",
      "21888/90000 [======>.......................] - ETA: 6s - loss: 0.1827 - acc: 0.9391\n",
      "22560/90000 [======>.......................] - ETA: 6s - loss: 0.1782 - acc: 0.9408\n",
      "23232/90000 [======>.......................] - ETA: 6s - loss: 0.1737 - acc: 0.9424\n",
      "23904/90000 [======>.......................] - ETA: 6s - loss: 0.1696 - acc: 0.9439\n",
      "24544/90000 [=======>......................] - ETA: 6s - loss: 0.1658 - acc: 0.9452\n",
      "25216/90000 [=======>......................] - ETA: 5s - loss: 0.1619 - acc: 0.9465\n",
      "25888/90000 [=======>......................] - ETA: 5s - loss: 0.1585 - acc: 0.9477\n",
      "26560/90000 [=======>......................] - ETA: 5s - loss: 0.1549 - acc: 0.9490\n",
      "27232/90000 [========>.....................] - ETA: 5s - loss: 0.1517 - acc: 0.9502\n",
      "27904/90000 [========>.....................] - ETA: 5s - loss: 0.1485 - acc: 0.9513\n",
      "28544/90000 [========>.....................] - ETA: 5s - loss: 0.1455 - acc: 0.9524\n",
      "29216/90000 [========>.....................] - ETA: 5s - loss: 0.1425 - acc: 0.9534\n",
      "29888/90000 [========>.....................] - ETA: 5s - loss: 0.1400 - acc: 0.9541\n",
      "30560/90000 [=========>....................] - ETA: 5s - loss: 0.1372 - acc: 0.9551\n",
      "31168/90000 [=========>....................] - ETA: 5s - loss: 0.1349 - acc: 0.9559\n",
      "31776/90000 [=========>....................] - ETA: 5s - loss: 0.1327 - acc: 0.9566\n",
      "32448/90000 [=========>....................] - ETA: 5s - loss: 0.1303 - acc: 0.9575\n",
      "33120/90000 [==========>...................] - ETA: 5s - loss: 0.1280 - acc: 0.9583\n",
      "33792/90000 [==========>...................] - ETA: 4s - loss: 0.1258 - acc: 0.9591\n",
      "34464/90000 [==========>...................] - ETA: 4s - loss: 0.1238 - acc: 0.9597\n",
      "35072/90000 [==========>...................] - ETA: 4s - loss: 0.1219 - acc: 0.9604\n",
      "35744/90000 [==========>...................] - ETA: 4s - loss: 0.1199 - acc: 0.9611\n",
      "36416/90000 [===========>..................] - ETA: 4s - loss: 0.1178 - acc: 0.9618\n",
      "37088/90000 [===========>..................] - ETA: 4s - loss: 0.1161 - acc: 0.9623\n",
      "37760/90000 [===========>..................] - ETA: 4s - loss: 0.1143 - acc: 0.9629\n",
      "38432/90000 [===========>..................] - ETA: 4s - loss: 0.1125 - acc: 0.9635\n",
      "39104/90000 [============>.................] - ETA: 4s - loss: 0.1107 - acc: 0.9641\n",
      "39776/90000 [============>.................] - ETA: 4s - loss: 0.1091 - acc: 0.9647\n",
      "40448/90000 [============>.................] - ETA: 4s - loss: 0.1074 - acc: 0.9653\n",
      "41120/90000 [============>.................] - ETA: 4s - loss: 0.1059 - acc: 0.9658\n",
      "41792/90000 [============>.................] - ETA: 4s - loss: 0.1043 - acc: 0.9664\n",
      "42464/90000 [=============>................] - ETA: 4s - loss: 0.1029 - acc: 0.9668\n",
      "43136/90000 [=============>................] - ETA: 4s - loss: 0.1014 - acc: 0.9673\n",
      "43808/90000 [=============>................] - ETA: 3s - loss: 0.1000 - acc: 0.9677\n",
      "44480/90000 [=============>................] - ETA: 3s - loss: 0.0986 - acc: 0.9682\n",
      "45152/90000 [==============>...............] - ETA: 3s - loss: 0.0973 - acc: 0.9687\n",
      "45824/90000 [==============>...............] - ETA: 3s - loss: 0.0960 - acc: 0.9691\n",
      "46496/90000 [==============>...............] - ETA: 3s - loss: 0.0947 - acc: 0.9695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47168/90000 [==============>...............] - ETA: 3s - loss: 0.0935 - acc: 0.9700\n",
      "47808/90000 [==============>...............] - ETA: 3s - loss: 0.0924 - acc: 0.9703\n",
      "48448/90000 [===============>..............] - ETA: 3s - loss: 0.0912 - acc: 0.9707\n",
      "49120/90000 [===============>..............] - ETA: 3s - loss: 0.0901 - acc: 0.9711\n",
      "49760/90000 [===============>..............] - ETA: 3s - loss: 0.0890 - acc: 0.9715\n",
      "50432/90000 [===============>..............] - ETA: 3s - loss: 0.0880 - acc: 0.9718\n",
      "51104/90000 [================>.............] - ETA: 3s - loss: 0.0869 - acc: 0.9721\n",
      "51776/90000 [================>.............] - ETA: 3s - loss: 0.0858 - acc: 0.9725\n",
      "52448/90000 [================>.............] - ETA: 3s - loss: 0.0849 - acc: 0.9728\n",
      "53120/90000 [================>.............] - ETA: 3s - loss: 0.0839 - acc: 0.9731\n",
      "53792/90000 [================>.............] - ETA: 3s - loss: 0.0829 - acc: 0.9735\n",
      "54432/90000 [=================>............] - ETA: 2s - loss: 0.0820 - acc: 0.9737\n",
      "55104/90000 [=================>............] - ETA: 2s - loss: 0.0812 - acc: 0.9740\n",
      "55776/90000 [=================>............] - ETA: 2s - loss: 0.0803 - acc: 0.9743\n",
      "56416/90000 [=================>............] - ETA: 2s - loss: 0.0794 - acc: 0.9746\n",
      "57088/90000 [==================>...........] - ETA: 2s - loss: 0.0785 - acc: 0.9749\n",
      "57760/90000 [==================>...........] - ETA: 2s - loss: 0.0777 - acc: 0.9751\n",
      "58432/90000 [==================>...........] - ETA: 2s - loss: 0.0768 - acc: 0.9754\n",
      "59104/90000 [==================>...........] - ETA: 2s - loss: 0.0760 - acc: 0.9757\n",
      "59776/90000 [==================>...........] - ETA: 2s - loss: 0.0752 - acc: 0.9759\n",
      "60448/90000 [===================>..........] - ETA: 2s - loss: 0.0744 - acc: 0.9762\n",
      "61088/90000 [===================>..........] - ETA: 2s - loss: 0.0738 - acc: 0.9764\n",
      "61760/90000 [===================>..........] - ETA: 2s - loss: 0.0730 - acc: 0.9767\n",
      "62432/90000 [===================>..........] - ETA: 2s - loss: 0.0722 - acc: 0.9769\n",
      "63104/90000 [====================>.........] - ETA: 2s - loss: 0.0715 - acc: 0.9771\n",
      "63776/90000 [====================>.........] - ETA: 2s - loss: 0.0708 - acc: 0.9773\n",
      "64448/90000 [====================>.........] - ETA: 2s - loss: 0.0701 - acc: 0.9776\n",
      "65120/90000 [====================>.........] - ETA: 2s - loss: 0.0694 - acc: 0.9778\n",
      "65792/90000 [====================>.........] - ETA: 2s - loss: 0.0688 - acc: 0.9780\n",
      "66464/90000 [=====================>........] - ETA: 1s - loss: 0.0681 - acc: 0.9782\n",
      "67104/90000 [=====================>........] - ETA: 1s - loss: 0.0675 - acc: 0.9784\n",
      "67744/90000 [=====================>........] - ETA: 1s - loss: 0.0669 - acc: 0.9786\n",
      "68416/90000 [=====================>........] - ETA: 1s - loss: 0.0662 - acc: 0.9788\n",
      "69088/90000 [======================>.......] - ETA: 1s - loss: 0.0656 - acc: 0.9790\n",
      "69728/90000 [======================>.......] - ETA: 1s - loss: 0.0651 - acc: 0.9792\n",
      "70400/90000 [======================>.......] - ETA: 1s - loss: 0.0645 - acc: 0.9794\n",
      "71072/90000 [======================>.......] - ETA: 1s - loss: 0.0639 - acc: 0.9796\n",
      "71744/90000 [======================>.......] - ETA: 1s - loss: 0.0633 - acc: 0.9798\n",
      "72416/90000 [=======================>......] - ETA: 1s - loss: 0.0628 - acc: 0.9800\n",
      "73088/90000 [=======================>......] - ETA: 1s - loss: 0.0622 - acc: 0.9801\n",
      "73760/90000 [=======================>......] - ETA: 1s - loss: 0.0617 - acc: 0.9803\n",
      "74432/90000 [=======================>......] - ETA: 1s - loss: 0.0612 - acc: 0.9805\n",
      "75104/90000 [========================>.....] - ETA: 1s - loss: 0.0606 - acc: 0.9807\n",
      "75776/90000 [========================>.....] - ETA: 1s - loss: 0.0601 - acc: 0.9808\n",
      "76448/90000 [========================>.....] - ETA: 1s - loss: 0.0596 - acc: 0.9810\n",
      "77120/90000 [========================>.....] - ETA: 1s - loss: 0.0591 - acc: 0.9812\n",
      "77792/90000 [========================>.....] - ETA: 0s - loss: 0.0586 - acc: 0.9813\n",
      "78464/90000 [=========================>....] - ETA: 0s - loss: 0.0582 - acc: 0.9815\n",
      "79136/90000 [=========================>....] - ETA: 0s - loss: 0.0577 - acc: 0.9816\n",
      "79808/90000 [=========================>....] - ETA: 0s - loss: 0.0572 - acc: 0.9818\n",
      "80448/90000 [=========================>....] - ETA: 0s - loss: 0.0568 - acc: 0.9819\n",
      "81088/90000 [==========================>...] - ETA: 0s - loss: 0.0563 - acc: 0.9821\n",
      "81760/90000 [==========================>...] - ETA: 0s - loss: 0.0559 - acc: 0.9822\n",
      "82432/90000 [==========================>...] - ETA: 0s - loss: 0.0555 - acc: 0.9823\n",
      "83072/90000 [==========================>...] - ETA: 0s - loss: 0.0550 - acc: 0.9825\n",
      "83744/90000 [==========================>...] - ETA: 0s - loss: 0.0546 - acc: 0.9826\n",
      "84416/90000 [===========================>..] - ETA: 0s - loss: 0.0543 - acc: 0.9827\n",
      "85088/90000 [===========================>..] - ETA: 0s - loss: 0.0538 - acc: 0.9828\n",
      "85760/90000 [===========================>..] - ETA: 0s - loss: 0.0534 - acc: 0.9830\n",
      "86432/90000 [===========================>..] - ETA: 0s - loss: 0.0531 - acc: 0.9831\n",
      "87072/90000 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9832\n",
      "87744/90000 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9833\n",
      "88416/90000 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9834\n",
      "89088/90000 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9835\n",
      "89760/90000 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9837\n",
      "90000/90000 [==============================] - 8s 83us/step - loss: 0.0511 - acc: 0.9837 - val_loss: 5.0068e-04 - val_acc: 1.0000\n",
      "Training model completed.\n",
      "Saving model files...\n",
      "model saved in ./outputs/model folder\n",
      "Saving model files completed.\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service.\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "1 items cleaning up...\n",
      "Cleanup took 0.00038242340087890625 seconds\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: deep-learning_1560263497_8dd19aee\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/fdbba0bc-f686-4b8b-8b29-394e0d9ae697/resourceGroups/Quick-Starts-Labs/providers/Microsoft.MachineLearningServices/workspaces/pipelines-workspace/experiments/deep-learning/runs/deep-learning_1560263497_8dd19aee\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'deep-learning_1560263497_8dd19aee',\n",
       " 'target': 'gpucluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2019-06-11T14:43:42.572558Z',\n",
       " 'endTimeUtc': '2019-06-11T14:47:02.477371Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'AzureML.DerivedImageName': 'azureml/azureml_d637b5dff0e8823965fb22958baf6ae4',\n",
       "  'ContentSnapshotId': '2a6cf9cc-b62b-4a96-8e84-d0eb47854be0',\n",
       "  'azureml.git.repository_uri': 'https://github.com/solliancenet/azure-machine-learning-quickstarts',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/solliancenet/azure-machine-learning-quickstarts',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '8cc7dcc52f111a222ff44fef3ed1367ea6b256c4',\n",
       "  'mlflow.source.git.commit': '8cc7dcc52f111a222ff44fef3ed1367ea6b256c4',\n",
       "  'azureml.git.dirty': 'True'},\n",
       " 'runDefinition': {'script': 'train.py',\n",
       "  'arguments': [],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpucluster',\n",
       "  'dataReferences': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment deep-learning Environment',\n",
       "   'version': 'Autosave_2019-06-11T14:31:42Z_b8162ad6',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['keras==2.2.4',\n",
       "        'azureml-defaults',\n",
       "        'tensorflow-gpu==1.13.1',\n",
       "        'horovod==0.16.1']},\n",
       "      'pandas'],\n",
       "     'channels': ['conda-forge']},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE',\n",
       "    'NCCL_SOCKET_IFNAME': '^docker0'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda10.0-cudnn7-ubuntu16.04',\n",
       "    'enabled': True,\n",
       "    'sharedVolumes': True,\n",
       "    'gpuSupport': True,\n",
       "    'shmSize': '1g',\n",
       "    'arguments': [],\n",
       "    'baseImageRegistry': {'address': None,\n",
       "     'username': None,\n",
       "     'password': None}},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False}},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'vmPriority': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None},\n",
       " 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/20_image_build_log.txt?sv=2018-03-28&sr=b&sig=R8zMFh0avQ9fgnH4VIl9rEwlgs1C5WKUfYpq3ueK1J0%3D&st=2019-06-11T14%3A37%3A03Z&se=2019-06-11T22%3A47%3A03Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/70_driver_log.txt?sv=2018-03-28&sr=b&sig=EZBzUzbFb3mlyPoI63o%2F%2FTW614jNPXG2SVZEiPU%2FSk4%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/driver_log.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/driver_log.txt?sv=2018-03-28&sr=b&sig=HLD5fdXlh2tO67idhfTyD1SNx1QeNKj7yRupbN6gO1A%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/55_batchai_stdout-job_post.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/55_batchai_stdout-job_post.txt?sv=2018-03-28&sr=b&sig=obDRjWIcejbeDFnQG1nvkG9JpgaVzSt5ZTwhEEgdzMY%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/56_batchai_stderr.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/56_batchai_stderr.txt?sv=2018-03-28&sr=b&sig=3sqkRkdkQdzUH7jmwQhnS6piR%2FqHQ%2BK486rLQJCgzp8%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/55_batchai_stdout.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/55_batchai_stdout.txt?sv=2018-03-28&sr=b&sig=nNkuwTb5v%2FspwW%2FAjc%2FnAa1Elrw4pJZQUFPII1cdh48%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/55_batchai_execution.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/55_batchai_execution.txt?sv=2018-03-28&sr=b&sig=fYTJV5pgVZAUstnZPEl9gUSmO1b8G0oHE9EP0QlwsNs%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'azureml-logs/55_batchai_stdout-job_prep.txt': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/azureml-logs/55_batchai_stdout-job_prep.txt?sv=2018-03-28&sr=b&sig=bm9tVzByyI7cjAMbXBe89DJOBZzfQ8ynkr4UHO6zZJw%3D&st=2019-06-11T14%3A37%3A04Z&se=2019-06-11T22%3A47%3A04Z&sp=r',\n",
       "  'logs/azureml/137_azureml.log': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/logs/azureml/137_azureml.log?sv=2018-03-28&sr=b&sig=VG7zG2zm%2BKVtpZBB%2Fub06QtFE1I26CLL5RNaMH%2FrYz8%3D&st=2019-06-11T14%3A37%3A03Z&se=2019-06-11T22%3A47%3A03Z&sp=r',\n",
       "  'logs/azureml/azureml.log': 'https://pipelinestoragecfc29ebdb.blob.core.windows.net/azureml/ExperimentRun/dcid.deep-learning_1560263497_8dd19aee/logs/azureml/azureml.log?sv=2018-03-28&sr=b&sig=c3VQqo2XtmklrI9TeftK7rkjkXtpnz4daKgra2uwT10%3D&st=2019-06-11T14%3A37%3A03Z&se=2019-06-11T22%3A47%3A03Z&sp=r'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the model files from the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, the Keras model is saved into two files, model.json and model.h5, in the outputs/models folder on the GPU cluster AmlCompute node. Azure ML automatically uploaded anything written in the ./outputs folder into run history file store. Subsequently, we can use the run object to download the model files. They are under the the outputs/model folder in the run history file store, and are downloaded into a local folder named model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from outputs/model/model.h5 to ./model/model.h5 ...\n"
     ]
    }
   ],
   "source": [
    "# create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model from model.h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 320,065\n",
      "Non-trainable params: 1,000,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('./model/model.h5')\n",
    "print(\"Model loaded from disk.\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evaluate how accurately the model performs against data it has not seen. Run the following cell to load the test data that was not used in either training or evaluating the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65 unique tokens.\n",
      "Shape of data tensor: (100000, 100)\n",
      "Shape of label tensor: (100000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load the car components labeled data\n",
    "car_components_df = pd.read_csv(data_url)\n",
    "components = car_components_df[\"text\"].tolist()\n",
    "labels = car_components_df[\"label\"].tolist()\n",
    "\n",
    "maxlen = 100                                           \n",
    "training_samples = 90000                                 \n",
    "validation_samples = 5000    \n",
    "max_words = 10000      \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(components)\n",
    "sequences = tokenizer.texts_to_sequences(components)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])                     \n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_test = data[training_samples + validation_samples:]\n",
    "y_test = labels[training_samples + validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see the accuracy on the test set (it is the second number in the array displayed, on a scale from 0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation will print the following metrics:  ['loss', 'acc']\n",
      "5000/5000 [==============================] - 0s 85us/step\n",
      "[0.00042561393415089697, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print('Model evaluation will print the following metrics: ', model.metrics_names)\n",
    "evaluation_metrics = model.evaluate(x_test, y_test)\n",
    "print(evaluation_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "Deep Learning",
  "notebookId": 2340934485665719
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
